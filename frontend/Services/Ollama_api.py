# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
# ‚ïë         üìÅ Python Project üìÅ
# ‚ïë
# ‚ïë  ‚ú® Team Members ‚ú®
# ‚ïë
# ‚ïë  üßë‚Äçüíª Elyasaf Cohen 311557227 üßë‚Äçüíª
# ‚ïë  üßë‚Äçüíª Eldad Cohen   207920711 üßë‚Äçüíª
# ‚ïë  üßë‚Äçüíª Israel Shlomo 315130344 üßë‚Äçüíª
# ‚ïë
# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

# =========== Load libraries ============ #
import requests
import os
from sentence_transformers import SentenceTransformer
from Frontend.Services.vector_store import load_data, create_faiss_index, search_similar_chunks
import google.generativeai as genai

# =========== Base function: send prompt to Ollama ============ #
def ask_ollama(prompt: str, model="gemma:2b") -> str:
    try:
        res = requests.post("http://localhost:11434/api/generate", json={
            "model": model,
            "prompt": prompt,
            "stream": False
        })
        data = res.json()

        if "error" in data:
            return f"‚ùå Ollama Error: {data['error']}"
        if "response" not in data:
            return f"‚ùå Unexpected reply format: {data}"

        return data["response"]

    except Exception as e:
        return f"‚ùå Exception: {e}"

# =========== Load the chunks and embeddings using dynamic path ============ #
current_dir = os.path.dirname(__file__)
json_path = os.path.join(current_dir, "Dataset", "embeddings.json")
chunks, embeddings = load_data(json_path)
index = create_faiss_index(embeddings)
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# =========== Contextual LLM interaction ============ #
def ask_ollama_contextual(question: str, model_name="gemma:2b") -> str:
    # =========== Step 1: Create embedding for the question ============ #
    question_vec = embedding_model.encode([question])[0]

    # =========== Step 2: Search for the most relevant chunks ============ #
    relevant_chunks = search_similar_chunks(question_vec, index, chunks, top_k=5)

    # =========== Step 3: Create the full prompt for Ollama ============ #
    context = "\n".join(relevant_chunks)
    final_prompt = f"Answer the question based on the following context:\n{context}\n\nQuestion: {question}"

    # =========== Step 4: Send the enriched prompt to Ollama ============ #
    return ask_ollama(final_prompt, model=model_name)


genai.configure(api_key= "AIzaSyAZA9yNT3ijGbsw_GwrJBpzC8Bi_u4aO_I")

def ask_google_gemini(prompt):
    try:
        model = genai.GenerativeModel(model_name="models/chat-bison-001")
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"‚ùå Error: {e}"